{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4rCKcndPybL"
   },
   "source": [
    "# Lab : Image Classification using Convolutional Neural Networks\n",
    "\n",
    "At the end of this laboratory, you would get familiarized with\n",
    "\n",
    "*   Creating deep networks using Keras\n",
    "*   Steps necessary in training a neural network\n",
    "*   Prediction and performance analysis using neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdglSzOi4Cp-"
   },
   "source": [
    "# **In case you use a colaboratory environment**\n",
    "By default, Colab notebooks run on CPU.\n",
    "You can switch your notebook to run with GPU.\n",
    "\n",
    "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
    "\n",
    "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
    "\n",
    "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wkicuxZdrdq"
   },
   "source": [
    "# **Working with a new dataset: CIFAR-10**\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
    "*   Convert the labels to one-hot encoded form.\n",
    "*   Normalize the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrb20KGMtTFq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# CIFAR-10 class labels\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create a figure\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "fig.suptitle(\"10 Random Samples from Each Class in CIFAR-10\", fontsize=16)\n",
    "\n",
    "# Plot 10 random samples from each class\n",
    "for class_idx in range(10):\n",
    "    # Get indices of images belonging to the current class\n",
    "    class_indices = np.where(y_train.flatten() == class_idx)[0]\n",
    "    \n",
    "    # Randomly select 10 images from this class\n",
    "    selected_indices = np.random.choice(class_indices, 10, replace=False)\n",
    "    \n",
    "    for i, img_idx in enumerate(selected_indices):\n",
    "        ax = axes[class_idx, i]\n",
    "        ax.imshow(x_train[img_idx])\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Print an example before and after one-hot encoding\n",
    "print(\"Original label:\", y_train[0])\n",
    "print(\"One-hot encoded label:\", y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images to the range [0,1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Print shape of the datasets\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape)\n",
    "print(\"One-hot encoded labels shape:\", y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ER5WlMNRydp"
   },
   "source": [
    "## Define the following model (same as the one in tutorial)\n",
    "\n",
    "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. \n",
    "\n",
    "Use the input as (32,32,3). \n",
    "\n",
    "The filter maps can then be flattened to provide features to the classifier. \n",
    "\n",
    "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfWCHxh8HGhN"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSN6riPISBMG"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    # Convolutional layer with 32 filters of size (3,3), ReLU activation\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    \n",
    "    # Max Pooling layer with pool size (2,2)\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    # Flatten the feature maps\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer with 100 neurons and ReLU activation\n",
    "    Dense(100, activation='relu'),\n",
    "    \n",
    "    # Classification layer with 10 neurons (one for each class) and softmax activation\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGtivbQJT39U"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn8UzPBZugVp"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # Suitable for multi-class classification\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),  # SGD with momentum\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train_one_hot,  # Training data and one-hot encoded labels\n",
    "    validation_data=(x_test, y_test_one_hot),  # Validate on test data\n",
    "    epochs=50,  # Train for 50 epochs\n",
    "    batch_size=512,  # Use a batch size of 512\n",
    "    verbose=1  # Print progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Plot the cross entropy loss curve and the accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and accuracy from training history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot Cross Entropy Loss Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cross Entropy Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2mrWK5hSB_o"
   },
   "source": [
    "## Defining Deeper Architectures: VGG Models\n",
    "\n",
    "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
    "\n",
    "Stack two convolutional layers with 32 filters, each of 3 x 3. \n",
    "\n",
    "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A80vLxW9FIek"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgca5dUNSFNc"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the images to range [0,1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the VGG-like model\n",
    "model_vgg = Sequential([\n",
    "    # First Convolutional Block (Two Conv Layers)\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    \n",
    "    # Max Pooling Layer\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully Connected Dense Layer with 128 neurons\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    # Classification Layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwaPphEBUtlC"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc2qtU0mUvVA"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# Compile the model\n",
    "model_vgg.compile(\n",
    "    loss='categorical_crossentropy',  # Multi-class classification loss\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),  # SGD optimizer with momentum\n",
    "    metrics=['accuracy']  # Track accuracy\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_vgg = model_vgg.fit(\n",
    "    x_train, y_train_one_hot,  # Training data and labels\n",
    "    validation_data=(x_test, y_test_one_hot),  # Validation data\n",
    "    epochs=50,  # Train for 50 epochs\n",
    "    batch_size=512,  # Batch size of 512\n",
    "    verbose=1  # Show training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2cRr2ZFSFds"
   },
   "source": [
    "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8OSHAf5SJPr"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# Extract loss and accuracy from both training histories\n",
    "loss_baseline = history.history['loss']\n",
    "val_loss_baseline = history.history['val_loss']\n",
    "accuracy_baseline = history.history['accuracy']\n",
    "val_accuracy_baseline = history.history['val_accuracy']\n",
    "\n",
    "loss_vgg = history_vgg.history['loss']\n",
    "val_loss_vgg = history_vgg.history['val_loss']\n",
    "accuracy_vgg = history_vgg.history['accuracy']\n",
    "val_accuracy_vgg = history_vgg.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss_baseline) + 1)\n",
    "\n",
    "# Plot Cross Entropy Loss Curves for both models\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_baseline, label='Baseline Model - Training Loss', linestyle='dashed')\n",
    "plt.plot(epochs, val_loss_baseline, label='Baseline Model - Validation Loss', linestyle='dashed')\n",
    "plt.plot(epochs, loss_vgg, label='VGG Model - Training Loss')\n",
    "plt.plot(epochs, val_loss_vgg, label='VGG Model - Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curves for both models\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy_baseline, label='Baseline Model - Training Accuracy', linestyle='dashed')\n",
    "plt.plot(epochs, val_accuracy_baseline, label='Baseline Model - Validation Accuracy', linestyle='dashed')\n",
    "plt.plot(epochs, accuracy_vgg, label='VGG Model - Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy_vgg, label='VGG Model - Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri9kU3wa3Rei"
   },
   "source": [
    "**Comment on the observation**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "... yes the deeper model performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzXmO1WoSKMY"
   },
   "source": [
    "*   Use predict function to predict the output for the test split\n",
    "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DObaoxhaSMUg"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the class probabilities for the test set\n",
    "y_pred_probs = model_vgg.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for VGG Model on CIFAR-10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUBrvRomU5O_"
   },
   "source": [
    "**Comment here :**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffwVz-FLSNG7"
   },
   "source": [
    "*    Print the test accuracy for the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4WX3_uLSN5I"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_vgg.evaluate(x_test, y_test_one_hot, verbose=1)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy of the VGG Model: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dySqfA6PVBjQ"
   },
   "source": [
    "## Define the complete VGG architecture.\n",
    "\n",
    "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer. \n",
    "\n",
    "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling. \n",
    "\n",
    "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
    "\n",
    "*   Change the size of input to 64 x 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm35siILFNT0"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH4lDVBuVA_Q"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Resize images from 32x32 to 64x64\n",
    "x_train_resized = np.array([cv2.resize(img, (64, 64)) for img in x_train])\n",
    "x_test_resized = np.array([cv2.resize(img, (64, 64)) for img in x_test])\n",
    "\n",
    "# Normalize the images to range [0,1]\n",
    "x_train_resized = x_train_resized.astype('float32') / 255.0\n",
    "x_test_resized = x_test_resized.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the deeper VGG-like model\n",
    "model_vgg_deep = Sequential([\n",
    "    # First Convolutional Block (64 filters)\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),  # Max Pooling\n",
    "    \n",
    "    # Second Convolutional Block (128 filters)\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),  # Max Pooling\n",
    "    \n",
    "    # Third Convolutional Block (256 filters)\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),  # Max Pooling\n",
    "    \n",
    "    # Flatten the output\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully Connected Dense Layer with 128 units\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    # Classification Layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model_vgg_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_B8kJGWhcM"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
    "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4elnDWnjEbmO"
   },
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "# Compile the VGG deep model\n",
    "model_vgg_deep.compile(\n",
    "    loss='categorical_crossentropy',  # Multi-class classification loss\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),  # SGD optimizer with momentum\n",
    "    metrics=['accuracy']  # Track accuracy\n",
    ")\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 512\n",
    "history_vgg_deep = model_vgg_deep.fit(\n",
    "    x_train_resized, y_train_one_hot,  # Training data and labels\n",
    "    validation_data=(x_test_resized, y_test_one_hot),  # Validation data\n",
    "    epochs=10,  # Train for 10 epochs\n",
    "    batch_size=512,  # Batch size of 512\n",
    "    verbose=1  # Show training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class probabilities for the test set\n",
    "y_pred_probs = model_vgg_deep.predict(x_test_resized)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Deep VGG Model on CIFAR-10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dlzFt0SXGDQ"
   },
   "source": [
    "# Understanding deep networks\n",
    "\n",
    "*   What is the use of activation functions in network? Why is it needed?\n",
    "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
    "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPy_1EWXX6fp"
   },
   "source": [
    "**Write the answers below :**\n",
    "\n",
    "1 - Use of activation functions:\n",
    "\n",
    "Activation Functions in Neural Networks\n",
    "An activation function introduces non-linearity into the neural network, allowing it to learn complex patterns and relationships in the data. Without activation functions, the network would behave like a simple linear model, limiting its ability to capture complex features.\n",
    "\n",
    "Why Are Activation Functions Needed?\n",
    "-Introduces Non-Linearity\n",
    "-Enables Deep Learning\n",
    "-Improves Model Performance\n",
    "_\n",
    "2 - Key Differences between sigmoid and softmax:\n",
    "\n",
    "-Sigmoid and Softmax are activation functions that help a neural network make decisions.\n",
    "-Sigmoid is used when we have two classes (binary classification). It outputs a probability between 0 and 1, which helps decide between class 0 and class 1.\n",
    "-Softmax is used when we have multiple classes (multi-class classification). It outputs probabilities for all classes, and the class with the highest probability is chosen.\n",
    "_\n",
    "\n",
    "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
    "\n",
    "Binary crossentropy is used for binary classification problems (two classes, e.g., Yes/No or 0/1). It works with a Sigmoid activation function, which outputs a single probability between 0 and 1, representing the likelihood of a positive class. Labels are represented as single values (0 or 1). In contrast, categorical crossentropy is used for multi-class classification (three or more classes). It works with a Softmax activation function, which outputs multiple probabilities for each class, ensuring that their sum equals 1. Labels must be one-hot encoded (e.g., [0,0,1,0] for class 3). Since CIFAR-10 has 10 classes, we use categorical crossentropy to correctly classify images into one of the 10 categories.\n",
    "_\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
